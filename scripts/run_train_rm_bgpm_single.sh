# export CUDA_VISIBLE_DEVICES=0

deepspeed train_rm_bgpm.py \
--save_path ../results/saved_model/2b_gemma2_bsmr04vhd8/rm \
--save_steps -1 \
--logging_steps 1 \
--eval_steps -1 \
--accumulated_gradient 1 \
--micro_train_batch_size 16 \
--pretrain google/gemma-2-2b-it \
--bf16 \
--max_epochs 2 \
--max_len 2048 \
--zero_stage 3 \
--learning_rate 2e-6 \
--general_preference_tau 0.1 \
--dataset natolambert/skywork-preferences-80k-v0.1-cleaned \
--dataset_probs 1 \
--flash_attn \
--gradient_checkpointing \
--group_size 1 \
--value_head_dim 12 \
--ptx_loss_coef 0.1 \
--train_split_ratio 1.0 \
--is_general_preference \
--is_bayesian_gpm \
--bayesian_kl_warmup_steps 200 \
--bayesian_max_kl_weight 0.002 \
--bayesian_prior_variance 0.03 \
--bayesian_init_logvar -4.0 \
--bayesian_min_logvar -8.0 \
--bayesian_max_logvar 1.0 \
--bayesian_sample_mix_ratio 0.4 \
--save_on_epoch_end \
--use_wandb True